{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ExpAn: Experiment Analysis\n",
    "\n",
    "ExpAn is a Python library for the statistical analysis of randomised controlled trials (A/B tests). \n",
    "\n",
    "The functions are standalone and can be imported and used from within other projects, and from the command line.\n",
    "\n",
    "The library is Open Source, published under the MIT license here:\n",
    "\n",
    "[github.com/zalando/expan](https://github.com/zalando/expan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions used in analysis \n",
    "\n",
    "1. Sample-size estimation:\n",
    "  * Treatment does not affect variance\n",
    "  * Variance in treatment and control is identical\n",
    "  * Mean of delta is normally distributed\n",
    "2. Welch t-test:\n",
    "  * Mean of means is t-distributed (or normally distributed) \n",
    "3. In general:\n",
    "  * Sample represents underlying population\n",
    "  * Entities are independent\n",
    "\n",
    "## Main user stories\n",
    "\n",
    "As a Data Scientist I want to perform all the basic analysis routines that are typical of a the analysis of an A/B Test (a.k.a. Between-Subject Randomised Control Trial) while retaining access to the raw data so I can perform very also custom analyses in order to answer the questions of stakeholders with little effort.\n",
    "\n",
    "As an analyst from a different department, I want to be able to bring my own data, and easily be able to use this library to perform analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Installation\n",
    "\n",
    "To install the library:\n",
    "\n",
    "    $ pip install expan\n",
    "\n",
    "For more information, start with the [README.rst](https://github.com/zalando/expan/blob/master/README.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ExpAn Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## `data.loaders` seperate details of data from the library\n",
    "\n",
    "Loaders read the raw data (e.g. **`csv_fetcher.py`**) and construct an `Experiment` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## `core.experiment` provides the analysis functionality\n",
    "\n",
    "**`Experiment`** class provides methods for the analysis of experiment data. Currently we support only the **`deltaKPI`** computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## `core.statistics` contains underlying statistical functions\n",
    "\n",
    "**`Statistics`** class provides methods for statistical computations such as: **`delta`** - computes the difference of means between the samples (x-y) with the confidence intervals, **`bootstrap`** - confidence intervals boostrapping, **`chi-square`** - chi-square homogeneity test on categorical data. \n",
    "\n",
    "The class is used by higher-level `experiment` module, and can be used directly from CLI, by passing in `Array`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## `core.binning` implements categorical and numerical binnings\n",
    "\n",
    "The class keeps binning separate from the data.\n",
    "\n",
    "**`Binning`** class has two subclasses `NumericalBinning` and `CategoricalBinning`. `NumericalBinning` groups data into numerical bins defined by numerical intervals. `CategoricalBinning` bins data into categories. This methods provides binning implementations which can be applied to unseed data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## `core.utils` contains supplied utility methods shared by other classes\n",
    "\n",
    "Currently it supports methods for generating random data for performing an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `core.version` constructs versioning of the package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data loaders\n",
    "\n",
    "Data loaders can be written as needed to handle different formats (CSV, Parquet, HDF5, etc) and different internal structures, so long as they return an `ExperimentData` object.\n",
    "\n",
    "Currently, only a simply CSV loader (`data.csv_fetcher`) has been implemented.\n",
    "\n",
    "We'll bypass this and work with synthesized data for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from expan.core.util import generate_random_data\n",
    "from os.path import dirname, join, realpath\n",
    "sys.path.insert(0, join(os.getcwd(), 'tests'))\n",
    "\n",
    "np.random.seed(0)\n",
    "data,metadata = generate_random_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>variant</th>\n",
       "      <th>normal_same</th>\n",
       "      <th>normal_shifted</th>\n",
       "      <th>feature</th>\n",
       "      <th>normal_shifted_by_feature</th>\n",
       "      <th>treatment_start_time</th>\n",
       "      <th>normal_unequal_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>-1.487862</td>\n",
       "      <td>-0.616148</td>\n",
       "      <td>non</td>\n",
       "      <td>-1.088533</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>-1.125186</td>\n",
       "      <td>1.783682</td>\n",
       "      <td>has</td>\n",
       "      <td>1.167307</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.565511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>0.388819</td>\n",
       "      <td>1.007539</td>\n",
       "      <td>non</td>\n",
       "      <td>-1.055948</td>\n",
       "      <td>1</td>\n",
       "      <td>6.704536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>-1.173873</td>\n",
       "      <td>-0.889252</td>\n",
       "      <td>non</td>\n",
       "      <td>-0.152459</td>\n",
       "      <td>4</td>\n",
       "      <td>1.209668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>1.112634</td>\n",
       "      <td>0.434377</td>\n",
       "      <td>has</td>\n",
       "      <td>0.175988</td>\n",
       "      <td>4</td>\n",
       "      <td>0.148207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity variant  normal_same  normal_shifted feature  \\\n",
       "0       0       A    -1.487862       -0.616148     non   \n",
       "1       1       B    -1.125186        1.783682     has   \n",
       "2       2       B     0.388819        1.007539     non   \n",
       "3       3       A    -1.173873       -0.889252     non   \n",
       "4       4       A     1.112634        0.434377     has   \n",
       "\n",
       "   normal_shifted_by_feature  treatment_start_time  normal_unequal_variance  \n",
       "0                  -1.088533                     7                 0.003991  \n",
       "1                   1.167307                     3                -3.565511  \n",
       "2                  -1.055948                     1                 6.704536  \n",
       "3                  -0.152459                     4                 1.209668  \n",
       "4                   0.175988                     4                 0.148207  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment': 'random_data_generation',\n",
       " 'primary_KPI': 'normal_shifted',\n",
       " 'source': 'simulated'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Constructing `Experiment` \n",
    "\n",
    "The `Experiment` class has the following parameters to construct an experiment:\n",
    "\n",
    "| Parameter | Description |\n",
    "|---|---|\n",
    "| **control_variant_name** | Indicates which of the variants is to be considered as a baseline (a.k.a. control). |\n",
    "| **data** | A data you want to run experiment for. An example of the data structure see above. |\n",
    "| **metadata** | Specifies an experiment name as the mandatory and data source as the optional fields. |\n",
    "| **report_kpi_names** | A list of strings specifying desired kpis to analyse (empty list by default). |\n",
    "| **derived_kpis** | A dictionary structured **{'name': ' `<`name_of_the_kpi`>`, 'formula': `<`formula_to_compute_kpi`>`}** (empty list by default) or a list of such dictionaries if more than 1 derived_kpi is wanted. **`<`name_of_the_kpi`>`**: name of the kpi. **`<`formula_to_compute_kpi`>`**: formula to calculate the desired kpi.|\n",
    "    \n",
    "**NOTE 1**. You should be careful specifying the correct structure to the derived_kpis dictionary including keys **'name'** and **'formula'**. Otherwise, construction of `Experiment` object will raise an exception.\n",
    "\n",
    "**NOTE 2**. Specify the derived kpi name in the **report_kpi_names** if you want to see the results for it too.\n",
    "\n",
    "**NOTE 3**. **data** must contain a column **entity**, a column **variant** and one column each for the kpis you defined.\n",
    "\n",
    "**NOTE 4**. Fields in **metadata** see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```metadata``` should contain the following fields. Optional fields are wrapped in brackets.\n",
    "\n",
    "| Field | Description |\n",
    "|---|---|\n",
    "|**`experiment`**| Name of the experiment, as known to stakeholders. It can be anything meaningful to you. |\n",
    "|**`[sources]`**| Names of the data sources used in the preparation of this data.|\n",
    "|**`[experiment_id]`**| This uniquely identifies the experiment. Could be a concatenation of the experiment name and the experiment start timestamp. |\n",
    "|**`[retrieval_time]`**| Time that data was fetched from original sources... perhaps this should be a list with entry per source? |\n",
    "|**`[primary_KPI]`**| Overall Evaluation Criteria. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expan.core.experiment import Experiment\n",
    "\n",
    "exp = Experiment(control_variant_name='A', \n",
    "                 data=data, \n",
    "                 metadata=metadata, \n",
    "                 report_kpi_names=['derived_kpi_one'],\n",
    "                 derived_kpis=[{'name':'derived_kpi_one','formula':'normal_same/normal_shifted'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment \"random_data_generation\" with 1 derived kpis, 1 report kpis, 10000 entities and 2 variants: *A*, B\n"
     ]
    }
   ],
   "source": [
    "print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrong input structure (e.g. missing derived_kpis dictionary keys or incorrect kpi keys) will raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Dictionary should have key \"formula\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-482dfafa7973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0mreport_kpi_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normal_shifted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'normal_same'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                   derived_kpis=[{'name':'derived_kpi_1'}])\n\u001b[0m",
      "\u001b[0;32m/Users/shuang/ZalandoWorkSpace/real_expan/expan/expan/core/experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, control_variant_name, data, metadata, report_kpi_names, derived_kpis)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Derived kpis should be an array of dictionaries'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'formula'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dictionary should have key \"formula\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'name'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dictionary should have key \"name\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Dictionary should have key \"formula\"'"
     ]
    }
   ],
   "source": [
    "exp = expan.experiment.Experiment(control_variant_name='A',\n",
    "                                  data=data, \n",
    "                                  metadata=metadata,\n",
    "                                  report_kpi_names=['normal_shifted', 'normal_same'],\n",
    "                                  derived_kpis=[{'name':'derived_kpi_1'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data we have two variants and one them is a baseline or control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants: {'A', 'B'}\n",
      "Control or baseline variant: A\n"
     ]
    }
   ],
   "source": [
    "print('Variants: {}'.format(exp.variant_names))\n",
    "print('Control or baseline variant: {}'.format(exp.control_variant_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Now we can start analysing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Let's start with a single delta analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.simplefilter('once', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warnings\": [\n",
      "    \"kpi: derived_kpi_one, variant: B: Sample variances differ too much to assume that population variances are equal.\"\n",
      "  ],\n",
      "  \"errors\": [],\n",
      "  \"expan_version\": \"0.6.2\",\n",
      "  \"control_variant\": \"A\",\n",
      "  \"kpis\": [\n",
      "    {\n",
      "      \"name\": \"derived_kpi_one\",\n",
      "      \"variants\": [\n",
      "        {\n",
      "          \"name\": \"A\",\n",
      "          \"delta_statistics\": {\n",
      "            \"delta\": 0.0,\n",
      "            \"confidence_interval\": [\n",
      "              {\n",
      "                \"percentile\": 2.5,\n",
      "                \"value\": -6.445256794169719\n",
      "              },\n",
      "              {\n",
      "                \"percentile\": 97.5,\n",
      "                \"value\": 6.445256794169717\n",
      "              }\n",
      "            ],\n",
      "            \"treatment_sample_size\": 6108,\n",
      "            \"control_sample_size\": 6108,\n",
      "            \"treatment_mean\": -4.572524000045541,\n",
      "            \"control_mean\": -4.572524000045541,\n",
      "            \"statistical_power\": 0.050000000000000044\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"B\",\n",
      "          \"delta_statistics\": {\n",
      "            \"delta\": 4.564575415240889,\n",
      "            \"confidence_interval\": [\n",
      "              {\n",
      "                \"percentile\": 2.5,\n",
      "                \"value\": -1.1450816040987393\n",
      "              },\n",
      "              {\n",
      "                \"percentile\": 97.5,\n",
      "                \"value\": 10.274232434580517\n",
      "              }\n",
      "            ],\n",
      "            \"treatment_sample_size\": 3892,\n",
      "            \"control_sample_size\": 6108,\n",
      "            \"treatment_mean\": -0.007948584804651233,\n",
      "            \"control_mean\": -4.572524000045541,\n",
      "            \"statistical_power\": 0.46900387352149797\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res_delta = exp.delta()\n",
    "print(json.dumps(res_delta, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently **`delta`** supports 4 methods to compute `delta`: `fixed_horizon` (default), `group_sequential`, `bayes_factor` and `bayes_precision`. All methods requires different additional parameters.\n",
    "\n",
    "**`fixed_horizon`** is a default method which has default settings/parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|---|---|\n",
    "|`assume_normal=True`| Specifies whether normal distribution assumptions can be made.|\n",
    "|`percentiles=[2.5, 97.5]`| A list of percentile values for confidence bounds.|\n",
    "|`min_observations=20`| Minimum number of observations needed.|\n",
    "|`nruns=10000`| Only used if assume normal is false.|\n",
    "|`relative=False`| If relative==True, then the values will be returned as distances below and above the mean, respectively, rather than the absolute values.|\n",
    "\n",
    "**`group_sequential`**:\n",
    "\n",
    "| Parameter | Description |\n",
    "|---|---|\n",
    "|`spending_function='obrien_fleming`| Currently we support only Obrient-Fleming alpha spending function for the frequentist early stopping decision.|\n",
    "|`estimated_sample_size=None`| Sample size to be achieved towards the end of experiment.|\n",
    "|`alpha=0.05`| Type-I error rate.|\n",
    "|`cap=8`| Upper bound of the adapted z-score.|\n",
    "\n",
    "\n",
    "**`bayes_factor`**:\n",
    "\n",
    "| Parameter | Description |\n",
    "|---|---|\n",
    "|`distribution='normal'`| The name of the KPI distribution model, which assumes a Stan model file with the same name exists.`*`\n",
    "|`num_iters=25000`| Number of iterations of bayes sampling.\n",
    "\n",
    "**`bayes_precision`**:\n",
    "\n",
    "| Parameter | Description |\n",
    "|---|---|\n",
    "|`distribution='normal'`| The name of the KPI distribution model, which assumes a Stan model file with the same name exists.`*`\n",
    "|`posterior_width=0.08`| The stopping criterion, threshold of the posterior width.\n",
    "|`num_iters=25000`| Number of iterations of bayes sampling.\n",
    "\n",
    "`*` currently we support **`normal`** and **`poisson`** models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to change any of the default values, just pass them as parameters to delta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta_freq = exp.delta(method='fixed_horizon', assume_normal=True, percentiles=[2.5, 99.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta_g_s = exp.delta(method='group_sequential', estimated_sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta_bayes_factor = exp.delta(method='bayes_factor', distribution='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warnings\": [\n",
      "    \"kpi: derived_kpi_one, variant: B: Sample variances differ too much to assume that population variances are equal.\"\n",
      "  ],\n",
      "  \"errors\": [],\n",
      "  \"expan_version\": \"0.6.2\",\n",
      "  \"control_variant\": \"A\",\n",
      "  \"kpis\": [\n",
      "    {\n",
      "      \"name\": \"derived_kpi_one\",\n",
      "      \"variants\": [\n",
      "        {\n",
      "          \"name\": \"A\",\n",
      "          \"delta_statistics\": {\n",
      "            \"stop\": true,\n",
      "            \"delta\": 0.0,\n",
      "            \"confidence_interval\": [\n",
      "              {\n",
      "                \"percentile\": 2.5,\n",
      "                \"value\": -8.396765530428699\n",
      "              },\n",
      "              {\n",
      "                \"percentile\": 97.5,\n",
      "                \"value\": 3.5794735964894677\n",
      "              }\n",
      "            ],\n",
      "            \"treatment_sample_size\": 6108,\n",
      "            \"control_sample_size\": 6108,\n",
      "            \"treatment_mean\": -4.572524000045541,\n",
      "            \"control_mean\": -4.572524000045541,\n",
      "            \"number_of_iterations\": 25000,\n",
      "            \"statistical_power\": 0.050000000000000044\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"B\",\n",
      "          \"delta_statistics\": {\n",
      "            \"stop\": true,\n",
      "            \"delta\": 4.564575415240889,\n",
      "            \"confidence_interval\": [\n",
      "              {\n",
      "                \"percentile\": 2.5,\n",
      "                \"value\": -2.8506067127900847\n",
      "              },\n",
      "              {\n",
      "                \"percentile\": 97.5,\n",
      "                \"value\": 8.497896742163277\n",
      "              }\n",
      "            ],\n",
      "            \"treatment_sample_size\": 3892,\n",
      "            \"control_sample_size\": 6108,\n",
      "            \"treatment_mean\": -0.007948584804651233,\n",
      "            \"control_mean\": -4.572524000045541,\n",
      "            \"number_of_iterations\": 25000,\n",
      "            \"statistical_power\": 0.46900387352149797\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(delta_bayes_factor, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the output\n",
    "\n",
    "| Metric | Description |\n",
    "|---|---|\n",
    "|**`treatment_mean`**| the mean of the treatment group |\n",
    "|**`control_mean`**| the mean of the control group |\n",
    "|**`control_sample_size`**| the sample size for the control group |\n",
    "|**`treatment_sample_size`**| the sample size for the treatment group |\n",
    "|**`delta`**| the difference between the treatment_mean and control_mean |\n",
    "|**`confidence_interval`**| the confidence interval: **`percentile`** - lower percentile and upper percentile; **`value`** - value for each percentile |\n",
    "|**`number_of_iterations`**| number of iterations used for bayes sampling for **`bayes_factor`** and **`bayes_precision`** methods|\n",
    "|**`stop`**| when **`True`** means the early stopping was done|\n",
    "|**`statistical_power`**| statistical power --- that is, the probability of a test to detect an effect, if the effect actually exists|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Using Bootstrapping:\n",
    "\n",
    "We implement boostrapping for data which is not normally distributed.\n",
    "\n",
    "We switch the flag 'assume_normal' to False for the `delta` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warnings\": [\n",
      "    \"kpi: derived_kpi_one, variant: B: Sample variances differ too much to assume that population variances are equal.\"\n",
      "  ],\n",
      "  \"errors\": [],\n",
      "  \"expan_version\": \"0.6.2\",\n",
      "  \"control_variant\": \"A\",\n",
      "  \"kpis\": [\n",
      "    {\n",
      "      \"name\": \"derived_kpi_one\",\n",
      "      \"variants\": [\n",
      "        {\n",
      "          \"name\": \"A\",\n",
      "          \"delta_statistics\": {\n",
      "            \"delta\": 0.0,\n",
      "            \"confidence_interval\": [\n",
      "              {\n",
      "                \"percentile\": 2.5,\n",
      "                \"value\": -6.451719231491042\n",
      "              },\n",
      "              {\n",
      "                \"percentile\": 97.5,\n",
      "                \"value\": 6.412118814781721\n",
      "              }\n",
      "            ],\n",
      "            \"treatment_sample_size\": 6108,\n",
      "            \"control_sample_size\": 6108,\n",
      "            \"treatment_mean\": -4.572524000045541,\n",
      "            \"control_mean\": -4.572524000045541,\n",
      "            \"statistical_power\": 0.050000000000000044\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"B\",\n",
      "          \"delta_statistics\": {\n",
      "            \"delta\": 4.564575415240889,\n",
      "            \"confidence_interval\": [\n",
      "              {\n",
      "                \"percentile\": 2.5,\n",
      "                \"value\": 0.06194124312058608\n",
      "              },\n",
      "              {\n",
      "                \"percentile\": 97.5,\n",
      "                \"value\": 9.032079819101977\n",
      "              }\n",
      "            ],\n",
      "            \"treatment_sample_size\": 3892,\n",
      "            \"control_sample_size\": 6108,\n",
      "            \"treatment_mean\": -0.007948584804651233,\n",
      "            \"control_mean\": -4.572524000045541,\n",
      "            \"statistical_power\": 0.46900387352149797\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res_delta = exp.delta(assume_normal=False)\n",
    "print(json.dumps(res_delta, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You may not notice here: bootstrapping takes considerably longer time than assuming the normality before running experiment. If we do not have an explicit reason to use it, it is almost always better to leave it off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binning\n",
    "\n",
    "You can use the Binning module to group data into subsets, i.e., assign each data into a corresponding `Bin` object. We will explain respectively in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bin object directly\n",
    "If you already know the set of bins you want to put data into. You can initialize a `bin` object directly.\n",
    "\n",
    "The first argument is the id of the bin. This might not be useful for your application but serves as a technical identifier.\n",
    "The second argument is the type of the bin. This can either be \"numerical\" or \"categorical\". Depending on the type, you should pass coresponding representation object as the third argument.\n",
    "\n",
    "We will explain the representation in next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExpAn core init: v0.6.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "bin: [0, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from expan.core.binning import *\n",
    "\n",
    "# numerical bin\n",
    "# create a numerical bin from value 0 (inclusive) to 10 (exclusive).\n",
    "Bin(\"numerical\", 0, 10, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "bin: ['a', 'b']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical bin\n",
    "# create a categorical bin which contains categories of \"a\" and \"b\".\n",
    "Bin(\"categorical\", [\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Create bin object automatically\n",
    "\n",
    "Given a number of bins, you can also create a list of bins from data by using the method ```create_bins(data, n_bins)```. \n",
    "\n",
    "It will create ```n_bins``` Bin ojbects, by which to separate the ```data``` as equally as possible. This method will also automatically detects numerical or categorical data, and creates corresponding bin representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = exp.data[exp.data.variant == 'A']\n",
    "data2 = exp.data[exp.data.variant == 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "bin: [-3.83665554846, -1.25906491145), \n",
      "bin: [-1.25906491145, -0.804751813719), \n",
      "bin: [-0.804751813719, -0.489466995342), \n",
      "bin: [-0.489466995342, -0.226662203724), \n",
      "bin: [-0.226662203724, 0.0239463824493), \n",
      "bin: [0.0239463824493, 0.276994331119), \n",
      "bin: [0.276994331119, 0.551060124216), \n",
      "bin: [0.551060124216, 0.868798338306), \n",
      "bin: [0.868798338306, 1.30062540106), \n",
      "bin: [1.30062540106, 4.47908425103]]\n"
     ]
    }
   ],
   "source": [
    "n_bins = 10\n",
    "create_bins(data1.normal_same, n_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Assign data to bins\n",
    "We can use the method ```apply(data):``` of the ```Bin``` object to assign data to one of the given bins.\n",
    "\n",
    "This method will return a subset of input data which belongs to this bin.\n",
    "It will return ```None``` if there is no data matched for this bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying bin to data in variant A:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4       1.112634\n",
       "6       0.085595\n",
       "10      0.335054\n",
       "13      0.542203\n",
       "15      0.002232\n",
       "19      0.467690\n",
       "21      1.171102\n",
       "23      1.289203\n",
       "27      0.141980\n",
       "28      0.313723\n",
       "31      1.345935\n",
       "37      2.418778\n",
       "41      0.288028\n",
       "44      0.411566\n",
       "46      1.120967\n",
       "47      0.805575\n",
       "48      0.975823\n",
       "49      0.008858\n",
       "54      1.352039\n",
       "57      2.159121\n",
       "58      0.091315\n",
       "61      1.637082\n",
       "63      0.735269\n",
       "66      1.030250\n",
       "71      0.644690\n",
       "77      0.723038\n",
       "78      0.085513\n",
       "83      1.889279\n",
       "84      0.238171\n",
       "89      0.580568\n",
       "          ...   \n",
       "9873    0.030269\n",
       "9875    0.863606\n",
       "9876    0.524865\n",
       "9880    0.008274\n",
       "9891    0.395712\n",
       "9900    1.168769\n",
       "9901    0.055230\n",
       "9903    0.192369\n",
       "9908    0.010693\n",
       "9909    0.354407\n",
       "9910    0.853060\n",
       "9914    0.492523\n",
       "9918    0.502002\n",
       "9924    1.096724\n",
       "9925    0.688108\n",
       "9934    0.367047\n",
       "9935    0.279812\n",
       "9936    0.445043\n",
       "9945    0.876760\n",
       "9948    0.261577\n",
       "9954    1.601119\n",
       "9955    1.797017\n",
       "9959    0.542985\n",
       "9969    0.206816\n",
       "9973    1.589447\n",
       "9980    0.130357\n",
       "9982    0.377618\n",
       "9985    0.193655\n",
       "9986    0.055740\n",
       "9987    0.664763\n",
       "Name: normal_same, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_bin = Bin(\"numerical\", 0, 10, True, False)\n",
    "print(\"applying bin to data in variant A:\")\n",
    "a_bin.apply(data1.normal_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying bin to data in variant B:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2       0.388819\n",
       "8       0.772848\n",
       "9       0.783160\n",
       "11      0.564789\n",
       "20      1.310606\n",
       "25      0.600733\n",
       "30      0.608267\n",
       "33      1.360168\n",
       "35      0.849585\n",
       "38      1.495458\n",
       "43      0.444854\n",
       "51      0.977872\n",
       "55      2.099408\n",
       "69      1.132805\n",
       "70      1.597397\n",
       "74      0.079915\n",
       "75      0.320930\n",
       "86      0.220631\n",
       "88      0.324758\n",
       "92      1.638961\n",
       "104     1.277857\n",
       "107     1.498012\n",
       "115     1.344854\n",
       "118     2.120994\n",
       "127     0.059905\n",
       "139     2.254038\n",
       "156     0.079048\n",
       "161     0.150602\n",
       "165     0.090310\n",
       "170     0.947512\n",
       "          ...   \n",
       "9862    0.725924\n",
       "9863    1.492610\n",
       "9864    0.908889\n",
       "9883    1.138699\n",
       "9885    0.167043\n",
       "9886    0.285282\n",
       "9887    0.322020\n",
       "9894    2.127297\n",
       "9897    1.896604\n",
       "9911    1.127925\n",
       "9913    0.499415\n",
       "9915    0.327819\n",
       "9927    0.729370\n",
       "9928    0.887623\n",
       "9937    0.278923\n",
       "9938    0.729843\n",
       "9940    0.201785\n",
       "9943    1.338250\n",
       "9957    0.544323\n",
       "9958    0.858663\n",
       "9971    0.290580\n",
       "9972    1.081581\n",
       "9977    0.460328\n",
       "9981    0.084888\n",
       "9983    0.443676\n",
       "9984    0.338594\n",
       "9989    1.544333\n",
       "9993    0.672613\n",
       "9996    0.792395\n",
       "9997    0.994518\n",
       "Name: normal_same, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"applying bin to data in variant B:\")\n",
    "a_bin.apply(data2.normal_same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Subgroup Analysis\n",
    "\n",
    "Subgroup analysis in ExaAn will select subgroup (which is a segment of data) based on the input argument, and then perform a regular delta analysis per subgroup as described before. \n",
    "That is to say, we don't compare between subgroups, but compare treatment with control within each subgroup.\n",
    "\n",
    "The input argument is a ```dict```, which maps feature name (key) to a list of ```Bin``` objects (value). This ```dict``` defines how and on which feature to perform the subgroup split. \n",
    "The returned value of subgroup analysis will be the result of regular delta analysis per subgroup.\n",
    "\n",
    "An example is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dimension': 'treatment_start_time',\n",
       "  'result': {'control_variant': 'A',\n",
       "   'errors': [],\n",
       "   'expan_version': '0.6.2',\n",
       "   'kpis': [{'name': 'derived_kpi_one',\n",
       "     'variants': [{'delta_statistics': {'confidence_interval': [{'percentile': 2.5,\n",
       "          'value': -1.5569210692070499},\n",
       "         {'percentile': 97.5, 'value': 2.1978673629800363}],\n",
       "        'control_mean': -0.32639393302612346,\n",
       "        'control_sample_size': 3076,\n",
       "        'delta': 0.3204731468864935,\n",
       "        'statistical_power': 0.095063282824786377,\n",
       "        'treatment_mean': -0.005920786139629961,\n",
       "        'treatment_sample_size': 1930},\n",
       "       'name': 'B'},\n",
       "      {'delta_statistics': {'confidence_interval': [{'percentile': 2.5,\n",
       "          'value': -2.1025221680926345},\n",
       "         {'percentile': 97.5, 'value': 2.102522168092634}],\n",
       "        'control_mean': -0.32639393302612346,\n",
       "        'control_sample_size': 3076,\n",
       "        'delta': 0.0,\n",
       "        'statistical_power': 0.050000000000000044,\n",
       "        'treatment_mean': -0.32639393302612346,\n",
       "        'treatment_sample_size': 3076},\n",
       "       'name': 'A'}]}],\n",
       "   'warnings': ['kpi: derived_kpi_one, variant: B: Sample variances differ too much to assume that population variances are equal.']},\n",
       "  'segment': '[0, 5)'},\n",
       " {'dimension': 'treatment_start_time',\n",
       "  'result': {'control_variant': 'A',\n",
       "   'errors': [],\n",
       "   'expan_version': '0.6.2',\n",
       "   'kpis': [{'name': 'derived_kpi_one',\n",
       "     'variants': [{'delta_statistics': {'confidence_interval': [{'percentile': 2.5,\n",
       "          'value': -6.9215226752839172},\n",
       "         {'percentile': 97.5, 'value': 0.14205372043176823}],\n",
       "        'control_mean': 3.379775978641749,\n",
       "        'control_sample_size': 3032,\n",
       "        'delta': -3.389734477426074,\n",
       "        'statistical_power': 0.59356839479094403,\n",
       "        'treatment_mean': -0.009958498784324924,\n",
       "        'treatment_sample_size': 1962},\n",
       "       'name': 'B'},\n",
       "      {'delta_statistics': {'confidence_interval': [{'percentile': 2.5,\n",
       "          'value': -4.017338312084763},\n",
       "         {'percentile': 97.5, 'value': 4.0173383120847621}],\n",
       "        'control_mean': 3.379775978641749,\n",
       "        'control_sample_size': 3032,\n",
       "        'delta': 0.0,\n",
       "        'statistical_power': 0.050000000000000044,\n",
       "        'treatment_mean': 3.379775978641749,\n",
       "        'treatment_sample_size': 3032},\n",
       "       'name': 'A'}]}],\n",
       "   'warnings': ['kpi: derived_kpi_one, variant: B: Sample variances differ too much to assume that population variances are equal.']},\n",
       "  'segment': '[5, 10)'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_to_bins = {\"treatment_start_time\": [\n",
    "    Bin(\"numerical\", 0, 5, True, False), \n",
    "    Bin(\"numerical\", 5, 10, True, False)]\n",
    "}\n",
    "exp.sga(dimension_to_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistics\n",
    "\n",
    "Here the underlying statistical functions are implemented. These are used by the higher-level `experiment` module, and can indeed be used directly by passing in NumPy `Array`s.\n",
    "\n",
    "The more interesting functions are:\n",
    "\n",
    "###  `bootstrap`\n",
    "\n",
    "Bootstraps the Confidence Intervals for a particular function comparing two samples. NaNs are ignored (discarded before calculation).\n",
    "\n",
    "This function, as well as others such as `normal_sample_difference`, and `delta`, take as input a list of percentiles, and return the values corresponding to those percentiles. This implementation is very general, allowing us to use the same functions for one-sided as well as two-sided tests, as well as more exactly recreating an output distribution (e.g. if we want to graphically depict more than 95% confidence intervals).\n",
    "\n",
    "### `delta`\n",
    "\n",
    "Uses either bootstrap or standard normal assumptions to compute the difference between two arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# That's it! Try it out for yourself:\n",
    "\n",
    "\n",
    "[github.com/zalando/expan](https://github.com/zalando/expan)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
